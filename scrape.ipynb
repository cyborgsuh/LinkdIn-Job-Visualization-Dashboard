{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cacc306",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4738c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1= pd.read_csv('Job Applications.csv')\n",
    "df_2= pd.read_csv('Job Applications_1.csv')\n",
    "df_3= pd.read_csv('Job Applications_2.csv')\n",
    "df_4= pd.read_csv('Job Applications_3.csv')\n",
    "df_5= pd.read_csv('Job Applications_4.csv')\n",
    "\n",
    "#Read all CSV files and concatenate them into a single DataFrame\n",
    "\n",
    "final_df= pd.concat([df_1, df_2, df_3, df_4, df_n], ignore_index=True)\n",
    "final_df['Application Date'] = pd.to_datetime(final_df['Application Date'], format='%m/%d/%y, %I:%M %p')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88006150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to scrape job details from LinkedIn\n",
    "def scrape_linkedin_job_details(job_url):\n",
    "    try:\n",
    "        response = requests.get(job_url)\n",
    "        if response.status_code != 200:\n",
    "            return None\n",
    "        \n",
    "        soup = bs4.BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract job title\n",
    "        title_element = soup.find('h1', class_='top-card-layout__title')\n",
    "        title = title_element.text.strip() if title_element else 'Unknown'\n",
    "        \n",
    "        # Extract company name\n",
    "        company_element = soup.find('a', class_='topcard__org-name-link')\n",
    "        company = company_element.text.strip() if company_element else 'Unknown'\n",
    "        \n",
    "        # Extract job description\n",
    "        description_element = soup.find('div', class_='description__text')\n",
    "        description = description_element.text.strip() if description_element else 'No description available'\n",
    "        \n",
    "        # Extract location\n",
    "        location_element = soup.find('span', class_='topcard__flavor--bullet')\n",
    "        location = location_element.text.strip() if location_element else 'Unknown'\n",
    "        \n",
    "        return {\n",
    "            'title': title,\n",
    "            'company': company,\n",
    "            'description': description,\n",
    "            'location': location\n",
    "            }\n",
    "        \n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error scraping {job_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "scrape_linkedin_job_details(final_df['Job Url'].iloc[0])  # Example usage with the first job URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47767248",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "all_jobs = []\n",
    "rows = []  # for pandas\n",
    "\n",
    "for i in tqdm(range(len(final_df))):\n",
    "    job_url = final_df['Job Url'].iloc[i]   # ⚡ you were still using final_df instead of final_df\n",
    "    job_details = scrape_linkedin_job_details(job_url)\n",
    "    \n",
    "    if job_details:\n",
    "        details_dict = job_details[1] if isinstance(job_details, list) else job_details\n",
    "        all_jobs.append({job_url: details_dict})\n",
    "        \n",
    "        # flatten for DataFrame\n",
    "        row = {\"link\": job_url}\n",
    "        row.update(details_dict)\n",
    "        rows.append(row)\n",
    "\n",
    "# Save JSON\n",
    "with open(\"job_descriptions.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(all_jobs, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# Save CSV\n",
    "df = pd.DataFrame(rows)\n",
    "df.to_csv(\"job_descriptions.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(\"Saved JSON and CSV ✅\")\n",
    "\n",
    "\n",
    "#These JSON and CSV files contain the scraped job details from LinkedIn which can be used for further analysis or visualization in the app"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
